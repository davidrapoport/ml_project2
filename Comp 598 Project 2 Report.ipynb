{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVMen - Comp 598 - Machine Learning - Project 2 Report\n",
    "\n",
    "---\n",
    "### David Rapoport, Ethan Macdonald, Charlie Bloomfield\n",
    "\n",
    "---\n",
    "### 1. Introduction\n",
    "In this report, we discuss our methods for classifying NPR interviews from their text content into one of four categories: Author Interviews, Movie Interviews, Music Interviews, and Interviews. Given an annotated dataset of 54,000 interviews, we implement three vectorizing techniques: Term Frequencyâ€“Inverse Document Frequency (TF-IDF), Bag of Words (BoW), and *n*-grams (NG). We then train several models: a Support Vector Classifier (SVC) using scikit-learn, Naive Bayes (NB) implemented by our team, and *k*-Nearest Neighbor (KNN) implemented by our team. Using standard cross validation techniques, we identify scikit-learn's SVC as the best classifier. We then tune our selection techniques to improve the classification performance of this model.\n",
    "\n",
    "---\n",
    "### 2. Running the Code\n",
    "\n",
    "In order to run our code, we execute main.py. When the program asks for a configuration file, we may enter a custom configuration file or simply leave the input as blank to use a default configuration file. A configuration file defines which vectorizors, feature selectors, and learners we wish to use.\n",
    "\n",
    "After we select a configuration file, the program displays a list of possible combinations to choose from. Each combination consists of a vectorizor, selector, and learner. To select every item, simply type 'a'. Once we have selected the combinations we wish to use, we enter the path to the data file we wish to use. To use the default path type 'd'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of the config file in python import notation.\n",
      " File must be in config directory. [Default config.default]\n",
      "\n",
      "0. count_vect percentile multinb\n",
      "1. count_vect percentile dec_tree\n",
      "2. count_vect percentile knn\n",
      "3. tfidf percentile multinb\n",
      "4. tfidf percentile dec_tree\n",
      "5. tfidf percentile knn\n",
      "\n",
      "\n",
      " Select a combination or a comma seperated list of combinations. 'a' will select every option\n",
      "0\n",
      "\n",
      "Enter the location of the test set file,\n",
      "If empty, no predictions will be output\n",
      "[d=data/ml_dataset_test_in.csv]\n",
      "d\n",
      "0.598130841121, count_vect percentile multinb\n",
      "Best combination is count_vect percentile multinb\n",
      "With parameters: \n",
      "\tpercentile__percentile: 25\n",
      "\tpercentile__score_func: <function chi2 at 0x10f3e0a28>\n",
      "\tcount_vect__ngram_range: (1, 2)\n",
      "\tcount_vect__max_df: 0.5\n",
      "\tcount_vect__min_df: 2\n",
      "\tcount_vect__binary: False\n"
     ]
    }
   ],
   "source": [
    "execfile(\"main.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program then trains a model for each of the selected combinations and outputs the accuracy of each model. At the end, the program returns information about the best model based on accuracy. This information includes which vectorizor, selector, learner and parameters were used.\n",
    "\n",
    "---\n",
    "### 3. Data Preprocessing\n",
    "For this project, we used three different vectorizors: TF-IDF, BoW, and NG. \n",
    "\n",
    "TF-IDF is a statistic used to quantify how important a word is in a document. As such, the TF-IDF value increases when a given word is found in a document, but this value is offset by the frequency of the word in the entire corpus. This is mathematically represented by the following equation:\n",
    "\n",
    "$$ \\mathrm{tfidf}(t,d,D) = \\log \\frac{N}{|\\{d \\in D: t \\in d\\}|} \\times \\left(0.5 + \\frac{0.5 \\times \\mathrm{f}(t, d)}{\\max\\{\\mathrm{f}(t, d):t \\in d\\}}\\right) $$\n",
    "\n",
    "BoW, on the other hand, counts how many times each word appears in a dataset without offsetting this value.\n",
    "\n",
    "---\n",
    "### 4. Feature Design and Selection\n",
    "David\n",
    "\n",
    "---\n",
    "### 5. Algorithm Selection\n",
    "\n",
    "##### 5.1 Naive Bayes\n",
    "David\n",
    "\n",
    "\n",
    "##### 5.2 K Nearest Neighbor\n",
    "We implemented K Nearest Neighbor as our standard algorithm, chosen for its implementation simplicity. \n",
    "\n",
    "\n",
    "---\n",
    "### 6. Optimization\n",
    "\n",
    "---\n",
    "### 7. Parameter Selection\n",
    "\n",
    "---\n",
    "### 8. Testing and Validation\n",
    "\n",
    "---\n",
    "### 9. Further Discussion\n",
    "\n",
    "---\n",
    "### 10. Legal Shit\n",
    "We hereby state that all the work presented in this report is that of the authors.\n",
    "\n",
    "---\n",
    "### 11. References\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
