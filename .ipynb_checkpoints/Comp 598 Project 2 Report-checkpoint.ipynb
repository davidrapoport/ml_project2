{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVMen - Comp 598 - Machine Learning - Project 2 Report\n",
    "\n",
    "---\n",
    "### David Rapoport, Ethan Macdonald, Charlie Bloomfield\n",
    "\n",
    "---\n",
    "### 1. Introduction\n",
    "In this report, we discuss our methods for classifying NPR interviews from their text content into one of four categories: Author Interviews, Movie Interviews, Music Interviews, and Interviews. Given an annotated dataset of 54,000 interviews, we implement three vectorizing techniques: Term Frequency–Inverse Document Frequency (TF-IDF), Bag of Words (BoW), and *n*-grams. We then train several models: a Support Vector Classifier (SVC) using scikit-learn, Naive Bayes (NB) implemented by our team, and *k*-Nearest Neighbor (KNN) implemented by our team. Using standard cross validation techniques, we identify scikit-learn's SVC as the best classifier. We then tune our selection techniques to improve the classification performance of this model.\n",
    "\n",
    "---\n",
    "### 2. Running the Code\n",
    "\n",
    "In order to run our code, we execute main.py. When the program asks for a configuration file, we may enter a custom configuration file or simply leave the input as blank to use a default configuration file. A configuration file defines which vectorizors, feature selectors, and learners we wish to use.\n",
    "\n",
    "After we select a configuration file, the program displays a list of possible combinations to choose from. Each combination consists of a vectorizor, selector, and learner. To select every item, simply type 'a'. Once we have selected the combinations we wish to use, we enter the path to the data file we wish to use. To use the default path type 'd'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of the config file in python import notation.\n",
      " File must be in config directory. [Default config.default]\n"
     ]
    }
   ],
   "source": [
    "execfile(\"main.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program then trains a model for each of the selected combinations and outputs the accuracy of each model. At the end, the program returns information about the best model based on accuracy (TODO: validation set accuracy? test set accuracy? cross validation?). This information includes which vectorizor, selector, learner and parameters were used.\n",
    "\n",
    "---\n",
    "### 3. Data Preprocessing\n",
    "The data is originally given as a CSV file containing id,text,label. Before creating different feature sets we first load the text and targets and perform preprocesssing on the text. Our preprocessing consists of several steps in which we normalize the data. First the text is tokenized using the nltk wordpunct tokenizer. After that we transform each token into lowercase, removing all non alphabetic characters (parentheses, commas, apostrophes, etc.), and remove words of length less than 3. This reason for this last step is that we want to remove the tail ends of contractions as well as any words such as \"be\", \"is\", \"am\" which are length two and are most likely irrelevant stop words. An example of the pipeline is shown below. After this we use the NLTK WordNetLemmatizer to lemmatize the word. We chose to lemmatize rather than stem because lemmatization is more likely to return a proper english word. This means we could later leverage the meaning of the word (for example with word2vec). The final step of the preprocessing was the remove stop words. The stop words used were the nltk stopwords with \"__EOS__\" appended to the list of stopwords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Shouldn', \"'\", 't', 'can', \"'\", 't', 'isn', \"'\", 't', 'doesn', \"'\", 't', '!!!']\n",
      "['shouldn', 'this', 'author', 'sing', 'isn', 'michael', 'jackson']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(wordpunct_tokenize(\"Shouldn't can't isn't doesn't!!!\"))\n",
    "tokens = wordpunct_tokenize(\"Shouldn't this author sing? He isn't Michael Jackson!!!\")\n",
    "print([lemmatizer.lemmatize(word.lower()) for word in tokens if len(word)>2 and word.isalpha()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4. Feature Design and Selection\n",
    "\n",
    "For this project, we used scikit_learn's CountVectorizer and TfidfVectorizer as well as our home grown Word2VecVectorizer for feature design. We used scikit_learn's SelectPercentile for feature selection. \n",
    "\n",
    "#### 4.1 Investigating the data\n",
    "\n",
    "Before beginning with feature design and selection it is useful to examine the train data we are given. We found that there were 54803 unigrams with 17722 of those unigrams only appearing in the corpus once. The top word was \"know\" appearing 62000 times, the next were \"like\", \"think\", \"one\", \"well\", \"people\", and \"really\" with frequencies rangings from 37000 - 23000. These words are very likely to be stopwords and future improvements may add these words to the stopword list or tweak with the __max_df__ parameter to remove them. \n",
    "\n",
    "Next, we found the 100 most frequent words from each class. We intersected these sets to get a list of 70 words which appear in the top 100 for all four classes. Again, in the future these could be used \n",
    "\n",
    "#### 4.2 Word2Vec\n",
    "\n",
    "#### 4.3 Results\n",
    "\n",
    "TF-IDF is a statistic used to quantify how important a word or phrase is in a document. As such, the TF-IDF value increases when a given word or phrase is found in a document, but this value is offset by the frequency of the word or phrase in the entire corpus. This is mathematically represented by the following equation:\n",
    "\n",
    "$$ \\mathrm{tfidf}(t,d,D) = \\log \\frac{N}{|\\{d \\in D: t \\in d\\}|} \\times \\left(0.5 + \\frac{0.5 \\times \\mathrm{f}(t, d)}{\\max\\{\\mathrm{f}(t, d):t \\in d\\}}\\right) $$\n",
    "\n",
    "BoW, on the other hand, counts how many times each word or phrase appears in a document without offsetting.\n",
    "\n",
    "For both BoW and TF-IDF we must decide whether to count unigrams, bigrams, trigrams or some combination thereof. Variance increases, and bias decreases, propotional to the length of the *n*-grams we consider. As such, it is important to select the right range of *n*-gram lengths to include in our vectors. The length of *n*-grams considered was independently determined for each combination of vectorizor, selector, and learner.\n",
    "\n",
    "TODO: Figures — comparison of BoW vs. TF-IDF in various scenarios. Comparison of unigram, bigram, trigram.\n",
    "David\n",
    "\n",
    "---\n",
    "### 5. Algorithm Selection\n",
    "\n",
    "##### 5.1 Naive Bayes\n",
    "David\n",
    "\n",
    "\n",
    "##### 5.2 *k*-Nearest Neighbor\n",
    "We implemented K Nearest Neighbor as our standard algorithm. K Nearest Neighbor involves  \n",
    "\n",
    "##### 5.3 SVM\n",
    "Finally, we test using scikit learn's Support Vector Classifier to classify the NPR articles. This implementation provides many model hyperparameters and optimization parameters. We explore two of these parameters: the penalty value for misclassifications and the kernel used by the SVC.\n",
    "\n",
    "Initial attempts to run SVC with the default parameters on the entire dataset proves to be infeasible on our machines due to runtime constraints. The polynomial runtime of SVC leads to a several hour runtime with no response. So we start again and try to assess it's classification percentage on 1% of the dataset. With the previous framework for validating on a single 80/20 train/test split, we get relatively impressive initial results with default parameters. Coupled with the optimum tfidf feature set, we see a 58.88% classification rate.\n",
    "\n",
    "--show graph\n",
    "\n",
    "With these initial results, we explore optimizing the misclassifcation penalty and kernel parameters to the SVC. We are within %10 range from our best Naive Bayes classification results and hope the optimizations might make the difference. To do so, we perform a search over the set of all scikit learn provided kernels [rbf\", \"linear\", \"poly\", \"sigmoid\", \"precomputed\"] with each of the C values [0.1, 1.0, 2.5, 5.0]. While the choice of kernels was to test out each of the provided kernels, the choice of C values was somewhat arbitrary. Testing over a range of values is optimal but grows combinatorially with the number of inputs and quickly becomes too expensive to execute.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### 6. Optimization\n",
    "\n",
    "---\n",
    "### 7. Parameter Selection\n",
    "In order to select optimal hyperparameters for our models, we employ GridSearchCV from scikit-learn, which exhaustively searches a manually defined range of hyperparameters. This search is guided by performance on a our validation set.\n",
    "\n",
    "TODO: Figures — comparison of various hyperparameters?\n",
    "\n",
    "---\n",
    "### 8. Testing and Validation\n",
    "\n",
    "---\n",
    "### 9. Further Discussion\n",
    "\n",
    "\n",
    "We hereby state that all the work presented in this report is that of the authors.\n",
    "\n",
    "---\n",
    "### 11. References\n",
    "\n",
    "TODO: scikit-learn reference\n",
    "\n",
    "TODO: David's refs from facebook?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
