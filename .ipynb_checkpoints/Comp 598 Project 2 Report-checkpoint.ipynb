{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVMen - Comp 598 - Machine Learning - Project 2 Report\n",
    "\n",
    "---\n",
    "### David Rapoport, Ethan Macdonald, Charlie Bloomfield\n",
    "\n",
    "---\n",
    "### 1. Introduction\n",
    "In this report, we discuss our methods for classifying NPR interviews from their text content into one of four categories: Author Interviews, Movie Interviews, Music Interviews, and Interviews. Given an annotated dataset of 54,000 interviews, we implement three vectorizing techniques: Term Frequency–Inverse Document Frequency (TF-IDF), Bag of Words (BoW), and *n*-grams. We then train several models: a Support Vector Classifier (SVC) using scikit-learn, Naive Bayes (NB) implemented by our team, and *k*-Nearest Neighbor (KNN) implemented by our team. Using standard cross validation techniques, we identify scikit-learn's SVC as the best classifier. We then tune our selection techniques to improve the classification performance of this model.\n",
    "\n",
    "---\n",
    "### 2. Running the Code\n",
    "\n",
    "In order to run our code, we execute main.py. When the program asks for a configuration file, we may enter a custom configuration file or simply leave the input as blank to use a default configuration file. A configuration file defines which vectorizors, feature selectors, and learners we wish to use.\n",
    "\n",
    "After we select a configuration file, the program displays a list of possible combinations to choose from. Each combination consists of a vectorizor, selector, and learner. To select every item, simply type 'a'. Once we have selected the combinations we wish to use, we enter the path to the data file we wish to use. To use the default path type 'd'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of the config file in python import notation.\n",
      " File must be in config directory. [Default config.default]\n"
     ]
    }
   ],
   "source": [
    "execfile(\"main.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program then trains a model for each of the selected combinations and outputs the accuracy of each model. At the end, the program returns information about the best model based on accuracy (TODO: validation set accuracy? test set accuracy? cross validation?). This information includes which vectorizor, selector, learner and parameters were used.\n",
    "\n",
    "---\n",
    "### 3. Data Preprocessing\n",
    "For this project, we used three different vectorizing techniques: TF-IDF, BoW, and *n*-gram. \n",
    "\n",
    "TF-IDF is a statistic used to quantify how important a word or phrase is in a document. As such, the TF-IDF value increases when a given word or phrase is found in a document, but this value is offset by the frequency of the word or phrase in the entire corpus. This is mathematically represented by the following equation:\n",
    "\n",
    "$$ \\mathrm{tfidf}(t,d,D) = \\log \\frac{N}{|\\{d \\in D: t \\in d\\}|} \\times \\left(0.5 + \\frac{0.5 \\times \\mathrm{f}(t, d)}{\\max\\{\\mathrm{f}(t, d):t \\in d\\}}\\right) $$\n",
    "\n",
    "BoW, on the other hand, counts how many times each word or phrase appears in a document without offsetting.\n",
    "\n",
    "For both BoW and TF-IDF we must decide whether to count unigrams, bigrams, trigrams or some combination thereof. Variance increases, and bias decreases, propotional to the length of the *n*-grams we consider. As such, it is important to select the right range of *n*-gram lengths to include in our vectors. The length of *n*-grams considered was independently determined for each combination of vectorizor, selector, and learner.\n",
    "\n",
    "TODO: Figures — comparison of BoW vs. TF-IDF in various scenarios. Comparison of unigram, bigram, trigram.\n",
    "\n",
    "---\n",
    "### 4. Feature Design and Selection\n",
    "David\n",
    "\n",
    "\n",
    "---\n",
    "### 5. Parameter Selection\n",
    "In order to select optimal hyperparameters for our models, we employ GridSearchCV from scikit-learn, which exhaustively searches a manually defined range of hyperparameters. This search is guided by performance on a our validation set.\n",
    "\n",
    "TODO: Figures — comparison of various hyperparameters?\n",
    "\n",
    "---\n",
    "### 6. Testing and Validation\n",
    "\n",
    "---\n",
    "### 7. Algorithm Selection\n",
    "\n",
    "##### 7.1 Naive Bayes\n",
    "David\n",
    "\n",
    "\n",
    "##### 7.2 *k*-Nearest Neighbor\n",
    "We implemented K Nearest Neighbor as our standard algorithm. K Nearest Neighbor involves  \n",
    "\n",
    "##### 7.3 SVM\n",
    "Finally, we test using scikit learn's Support Vector Classifier to classify the NPR articles. This implementation provides many model hyperparameters and optimization parameters. We explore two of these parameters: the penalty value for misclassifications and the kernel used by the SVC.\n",
    "\n",
    "Initial attempts to run SVC with the default parameters on the entire dataset proves to be infeasible on our machines due to runtime constraints. The polynomial runtime of SVC leads to a several hour runtime with no response. So we start again and try to assess it's classification percentage on 1% of the dataset. With the previous framework for validating on a single 80/20 train/test split, we get relatively impressive initial results with default parameters. Coupled with the optimum tfidf feature set, we see a 58.88% classification rate.\n",
    "\n",
    "--show graph\n",
    "\n",
    "With these initial results, we explore optimizing the misclassifcation penalty and kernel parameters to the SVC. We are within %10 range from our best Naive Bayes classification results and hope the optimizations might make the difference. To do so, we perform a search over the set of all scikit learn's provided kernels [rbf\", \"linear\", \"poly\", \"sigmoid\", \"precomputed\"] with each of the C values [0.1, 1.0, 2.5, 5.0]. While the choice of kernels was to test out each of the provided kernels, the choice of C values was somewhat arbitrary. Testing over a range of values is optimal but grows combinatorially with the number of inputs and quickly becomes too expensive to execute.\n",
    "\n",
    "\n",
    "--show graph of svc combinations\n",
    "\n",
    "With the insight we gained from running SVC on 1% of the data, we are ready to train on all the data and try make a submission to see how our training generalizes to test data. We are still running into problems with running SVC on the entire dataset, so we choose to run our code on a remote server provided by [Cloud Digital](https://www.digitalocean.com/) with improved processing power. We install and run our program to train the SVC model overnight, but an entire night of execution on the remote server proves to be insufficient to train the model and we terminate it's execution. This is somewhat surprising at the time, though reconsidering the polynomial runtime suggests that \n",
    "\n",
    "Unable to complete model training, we do not use SVC as a . Because it's test results show classification percentages lower then Naive Bayes, we do not use it in making a submission.\n",
    "\n",
    "---\n",
    "### 8. Optimization\n",
    "\n",
    "\n",
    "---\n",
    "### 9. Further Discussion\n",
    "\n",
    "\n",
    "---\n",
    "### 10. Statement of Faith\n",
    "We hereby state that all the work presented in this report is that of the authors.\n",
    "\n",
    "---\n",
    "### 11. References\n",
    "\n",
    "TODO: scikit-learn reference\n",
    "\n",
    "TODO: David's refs from facebook?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
